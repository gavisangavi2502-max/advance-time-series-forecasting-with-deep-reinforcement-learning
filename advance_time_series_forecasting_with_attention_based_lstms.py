# -*- coding: utf-8 -*-
"""advance time series forecasting with attention based LSTMs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dbG0I5FBvCOxJIch-hgjhWK2q4JL5BEB
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import mean_squared_error, mean_absolute_error

# -----------------------------
# Synthetic multivariate dataset
# -----------------------------
def generate_synthetic_data(n_samples=3000):
    t = np.linspace(0, 100, n_samples)

    feature1 = np.sin(t) + np.random.normal(0, 0.1, n_samples)
    feature2 = np.cos(t/2) + np.random.normal(0, 0.1, n_samples)
    feature3 = np.sin(t/3) * np.cos(t/5) + np.random.normal(0, 0.1, n_samples)
    feature4 = np.log(t+1) + np.random.normal(0, 0.05, n_samples)
    feature5 = np.sin(t*1.5) + np.cos(t*0.8) + np.random.normal(0, 0.1, n_samples)

    df = pd.DataFrame({
        "f1": feature1,
        "f2": feature2,
        "f3": feature3,
        "f4": feature4,
        "f5": feature5
    })

    # Target = future value of f1 (forecast 7 days ahead)
    df["target"] = df["f1"].shift(-7)
    df.dropna(inplace=True)
    return df

df = generate_synthetic_data()

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Scaling
scaler = MinMaxScaler()
scaled = scaler.fit_transform(df)

# Windowing function
def create_sequences(data, seq_len=60):
    xs, ys = [], []
    for i in range(len(data) - seq_len):
        x = data[i:i+seq_len, :-1]
        y = data[i+seq_len, -1]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

SEQ_LEN = 60
X, y = create_sequences(scaled, SEQ_LEN)

# Train / Test Split
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

class TimeSeriesDS(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self): return len(self.X)
    def __getitem__(self, i): return self.X[i], self.y[i]

train_loader = DataLoader(TimeSeriesDS(X_train, y_train), batch_size=64, shuffle=True)
test_loader  = DataLoader(TimeSeriesDS(X_test, y_test), batch_size=64)

class BaselineLSTM(nn.Module):
    def __init__(self, input_dim=5, hidden_dim=64, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = out[:, -1, :]
        return self.fc(out)

baseline = BaselineLSTM()
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(baseline.parameters(), lr=0.001)

def train_model(model, loader, epochs=10):
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for Xb, yb in loader:
            optimizer.zero_grad()
            preds = model(Xb).squeeze()
            loss = criterion(preds, yb)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1} | Loss: {total_loss/len(loader):.4f}")

train_model(baseline, train_loader, epochs=10)

class SelfAttention(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.attn = nn.Linear(hidden_dim, 1)

    def forward(self, lstm_out):
        weights = torch.softmax(self.attn(lstm_out), dim=1)
        context = (weights * lstm_out).sum(dim=1)
        return context

class AttentionLSTM(nn.Module):
    def __init__(self, input_dim=5, hidden_dim=64, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.attention = SelfAttention(hidden_dim)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        context = self.attention(lstm_out)
        return self.fc(context)

att_model = AttentionLSTM()
att_optimizer = torch.optim.Adam(att_model.parameters(), lr=0.001)

train_model(att_model, train_loader, epochs=10)

def evaluate(model, loader):
    model.eval()
    preds, actual = [], []
    with torch.no_grad():
        for Xb, yb in loader:
            pred = model(Xb).squeeze()
            preds.extend(pred.numpy())
            actual.extend(yb.numpy())

    mse = mean_squared_error(actual, preds)
    rmse = np.sqrt(mse)
    mae  = mean_absolute_error(actual, preds)
    return rmse, mae

base_rmse, base_mae = evaluate(baseline, test_loader)
att_rmse, att_mae   = evaluate(att_model, test_loader)

print("Baseline LSTM:", base_rmse, base_mae)
print("Attention LSTM:", att_rmse, att_mae)

metrics_table = pd.DataFrame({
    "Model": ["Baseline LSTM", "Attention LSTM"],
    "RMSE": [base_rmse, att_rmse],
    "MAE" : [base_mae, att_mae]
})

print(metrics_table)

att_model.eval()
last_seq = torch.tensor(X_test[-1:]).float()
future_pred = att_model(last_seq).item()
print("Final Forecast Value:", future_pred)

!pip install reportlab
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib.pagesizes import A4
import os

# Generate dataset
def gen():
    t=np.linspace(0,100,300)
    df=pd.DataFrame({"t":t,"f1":np.sin(t)})
    return df

df=gen()

# Create the directory if it doesn't exist
os.makedirs('/mnt/data', exist_ok=True)

csv_path="/mnt/data/dataset.csv"
df.to_csv(csv_path,index=False)

# Plot
plt.figure()
plt.plot(df["t"],df["f1"])
png_path="/mnt/data/plot.png"
plt.savefig(png_path)

# PDF
pdf_path="/mnt/data/report.pdf"
styles=getSampleStyleSheet()
doc=SimpleDocTemplate(pdf_path,pagesize=A4)
story=[Paragraph("Time Series Report",styles["Title"]),Spacer(1,20),
       Paragraph("Sample synthetic dataset and plot.",styles["Normal"])]
doc.build(story)