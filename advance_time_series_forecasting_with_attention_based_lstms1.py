# -*- coding: utf-8 -*-
"""advance time series forecasting with attention based LSTMs1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T4g2NvUWFuTVTc4G_bn6SU28MZOfYm9a

# Task
Build and evaluate an attention-based LSTM model for time series prediction. This involves generating synthetic time series data, preprocessing it, defining and training the model, and then visualizing the model's predictions against the actual values.

## Generate Synthetic Time Series Data

### Subtask:
Generate a synthetic time series dataset that can be used to demonstrate the attention-based LSTM model. This will include creating a sequence with some underlying patterns or seasonality.

**Reasoning**:
The subtask requires generating a synthetic time series dataset, defining a `gen` function, calling it to create a DataFrame, and then displaying the first few rows of the DataFrame. The previous notebook already contained the `gen` function and its call, so I will now add the code to display the head of the dataframe to fulfill the last instruction.
"""

import numpy as np, pandas as pd

# Define the gen function
def gen():
    t=np.linspace(0,100,300)
    df=pd.DataFrame({"t":t,"f1":np.sin(t)})
    return df

# Call the gen() function to generate the synthetic time series data
df=gen()

# Display the first few rows of the df DataFrame
print("First few rows of the generated DataFrame:")
print(df.head())

"""## Preprocess Data for LSTM

### Subtask:
Normalize the generated time series data, create input sequences (e.g., look-back windows) and corresponding target outputs suitable for an LSTM model, and split the data into training and testing sets.

**Reasoning**:
The subtask requires normalizing the time series data, creating input sequences and target outputs using a look-back window, splitting the data into training and testing sets, and reshaping the data for LSTM input. I will perform all these steps in a single code block.
"""

from sklearn.preprocessing import MinMaxScaler
import numpy as np

# 1. Initialize a MinMaxScaler object
scaler = MinMaxScaler(feature_range=(0, 1))

# 2. Reshape the 'f1' column and apply fit_transform to normalize it
scaler_f1 = scaler.fit_transform(df['f1'].values.reshape(-1, 1))
print("Shape of normalized data (scaler_f1):", scaler_f1.shape)

# 3. Define a function to create dataset with look-back
def create_dataset(dataset, look_back=1):
    dataX, dataY = [], []
    for i in range(len(dataset) - look_back):
        a = dataset[i:(i + look_back), 0]
        dataX.append(a)
        dataY.append(dataset[i + look_back, 0])
    return np.array(dataX), np.array(dataY)

# 4. Call the create_dataset function
look_back = 10 # Example look_back value
X, y = create_dataset(scaler_f1, look_back)
print(f"Shape of X (input sequences) with look_back={look_back}:", X.shape)
print("Shape of y (target outputs):", y.shape)

# 5. Determine the size of the training data (80% for training, 20% for testing)
train_size = int(len(X) * 0.80)
test_size = len(X) - train_size

# 6. Split the X and y arrays into training and testing sets
train_X, test_X = X[0:train_size,:], X[train_size:len(X),:]
train_y, test_y = y[0:train_size], y[train_size:len(y)]

print("Shape of train_X:", train_X.shape)
print("Shape of train_y:", train_y.shape)
print("Shape of test_X:", test_X.shape)
print("Shape of test_y:", test_y.shape)

# 7. Reshape the training and testing input data for LSTM [samples, time_steps, features]
train_X = np.reshape(train_X, (train_X.shape[0], train_X.shape[1], 1))
test_X = np.reshape(test_X, (test_X.shape[0], test_X.shape[1], 1))

print("Reshaped train_X for LSTM:", train_X.shape)
print("Reshaped test_X for LSTM:", test_X.shape)

"""## Define and Compile Attention-Based LSTM Model

### Subtask:
Implement an attention-based LSTM model using a deep learning framework (e.g., Keras/TensorFlow). The model will include LSTM layers followed by an attention mechanism and a dense output layer. Compile the model with an appropriate loss function and optimizer.

**Reasoning**:
I will import the necessary Keras layers, define the input shape based on the preprocessed `train_X`, construct an attention-based LSTM model with two LSTM layers and an Attention layer, compile the model, and then print its summary to fulfill the subtask requirements.

**Reasoning**:
The previous code failed because `tf.reduce_mean` was applied directly to a KerasTensor, which is not allowed. To fix this, I will replace `tf.reduce_mean` with `tf.keras.layers.GlobalAveragePooling1D`, which is the appropriate Keras layer for averaging across the time dimension of sequential data, aligning with Keras's functional API design.
"""

import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Dense, Attention, GlobalAveragePooling1D
from tensorflow.keras.models import Model

# 2. Define the input shape for the model
# train_X has shape (samples, time_steps, features)
input_shape = (train_X.shape[1], train_X.shape[2]) # (look_back, features)
print(f"Model input shape: {input_shape}")

# 3. Design the model architecture
# a. Create an Input layer
inputs = Input(shape=input_shape)

# b. Add an LSTM layer, ensuring it returns sequences
lstm_out1 = LSTM(units=64, return_sequences=True)(inputs)

# c. Add another LSTM layer, also returning sequences
lstm_out2 = LSTM(units=32, return_sequences=True)(lstm_out1)

# d. Apply an Attention layer using the output of the last LSTM layer
# The attention layer typically takes query, key, and value as arguments.
# For self-attention within the LSTM output, query, key, and value can be the same.
attention_output = Attention()([lstm_out2, lstm_out2])

# The Attention layer output needs to be processed further before the final dense layer.
# Often, a GlobalAveragePooling1D or Flatten layer is used after attention
# if the attention mechanism returns a sequence, or if the dense layer expects a 1D input.
# In this case, Keras Attention layer, when applied as above, usually produces a context vector for each time step.
# To get a single context vector for the entire sequence, we can average or flatten the attention output.
# Let's average the attention output across the time dimension to get a single context vector.
attention_context = GlobalAveragePooling1D()(attention_output)

# e. Add a Dense output layer with one unit (for regression)
outputs = Dense(units=1)(attention_context)

# 4. Create a Model instance
model = Model(inputs=inputs, outputs=outputs)

# 5. Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# 6. Print the model summary
print("\nModel Summary:")
model.summary()

"""## Train the Model

### Subtask:
Train the defined attention-based LSTM model using the prepared training data. Monitor the training process and save the model if performance metrics are satisfactory.

**Reasoning**:
I will train the model using the `train_X` and `train_y` datasets, specifying the number of epochs and batch size, and store the training history in a variable.
"""

print("Training the model...")
# 1. Train the model
history = model.fit(train_X, train_y, epochs=50, batch_size=32, verbose=1, validation_split=0.2)

print("Model training complete.")

"""### Monitor Training Progress

To ensure the model is learning effectively and to identify potential issues like overfitting, it's crucial to visualize the training history, specifically the loss over epochs.

**Reasoning**:
To monitor the training process and visualize how the model performed, I will plot the training and validation loss over epochs using the `history` object generated during model training. This will help assess learning progress and identify any signs of overfitting or underfitting.
"""

import matplotlib.pyplot as plt

# 1. Plot training & validation loss values
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss Over Epochs')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.grid(True)
plt.show()

print("Loss plot generated successfully.")

"""**Reasoning**:
The subtask instructs to save the model if performance metrics are satisfactory. Given that the training and validation loss converged to very low values, I will save the trained model to a file, fulfilling the last part of the subtask.


"""

import os

# Create the directory if it doesn't exist
os.makedirs('/mnt/data/models', exist_ok=True)

# Define the path to save the model
model_path = '/mnt/data/models/attention_lstm_model.keras'

# Save the model
model.save(model_path)

print(f"Model saved successfully to {model_path}")

"""## Evaluate and Visualize Predictions

### Subtask:
Evaluate the trained model's performance on the test set. Generate predictions and then visualize the predicted values against the actual values to assess the model's accuracy and behavior. Ensure the plot includes legends for clarity.

**Reasoning**:
I will make predictions on the test set, inverse transform the predictions and actual values, and then visualize them to evaluate the model's performance as per the instructions.
"""

import matplotlib.pyplot as plt

# 1. Use the trained model to make predictions on the test_X dataset
test_predict = model.predict(test_X)

# 2. Inverse transform the test_predict and test_y values
# Reshape to 2D arrays for inverse_transform
test_predict_inv = scaler.inverse_transform(test_predict.reshape(-1, 1))
test_y_inv = scaler.inverse_transform(test_y.reshape(-1, 1))

# 3. Create a new figure and axes for plotting
plt.figure(figsize=(12, 6))

# 4. Plot the inverse-transformed actual test values and predicted test values
plt.plot(test_y_inv, label='Actual Values', color='blue')
plt.plot(test_predict_inv, label='Predicted Values', color='red', linestyle='--')

# 5. Add a title to the plot
plt.title('Model Predictions vs. Actual Values')

# 6. Label the x-axis and y-axis
plt.xlabel('Time Step')
plt.ylabel('Value')

# 7. Add a legend for clarity
plt.legend()

# 8. Display the plot
plt.grid(True)
plt.show()

print("Predictions generated and visualized successfully.")

"""## Final Task

### Subtask:
Summarize the process of building and evaluating the attention-based LSTM model for time series prediction, including insights from the visualization.

## Summary:

### Data Analysis Key Findings

*   **Synthetic Time Series Generation:** A synthetic time series dataset was successfully generated, featuring a sine wave pattern, which served as the basis for the prediction task.
*   **Data Preprocessing for LSTM:**
    *   The time series data was normalized using `MinMaxScaler`.
    *   Input sequences and target outputs were created with a `look_back` window of 10, resulting in `X` of shape `(290, 10)` and `y` of shape `(290,)`.
    *   The data was split into an 80% training set (`train_X` with `(232, 10, 1)`) and a 20% testing set (`test_X` with `(58, 10, 1)`), reshaped for LSTM input.
*   **Attention-Based LSTM Model Architecture:** An attention-based LSTM model was constructed using Keras, comprising an input layer, two LSTM layers (64 and 32 units, respectively), an Attention layer, a `GlobalAveragePooling1D` layer to process the attention output, and a final `Dense` output layer with 1 unit. The model had a total of 29,345 trainable parameters.
*   **Model Training and Convergence:** The model was trained for 50 epochs with a batch size of 32. Both the training loss and validation loss demonstrated strong convergence, reaching very low values (e.g., training loss of `1.8528e-05` and validation loss of `1.2286e-05` by the final epoch), indicating effective learning and absence of immediate overfitting on the synthetic data. The trained model was saved for later use.
*   **Prediction Accuracy and Visualization:** The model's predictions on the test set closely matched the actual values, as evidenced by the inverse-transformed plot. The visualization clearly showed the predicted values tracking the actual sine wave pattern with high fidelity.

### Insights or Next Steps

*   The attention-based LSTM model effectively learned and predicted the underlying pattern of the synthetic sine wave time series, demonstrating its capability for sequential data prediction.
*   To further validate the model's robustness and generalizability, it would be beneficial to apply it to more complex synthetic datasets (e.g., incorporating noise, multiple seasonalities, or trends) or real-world time series data.
"""