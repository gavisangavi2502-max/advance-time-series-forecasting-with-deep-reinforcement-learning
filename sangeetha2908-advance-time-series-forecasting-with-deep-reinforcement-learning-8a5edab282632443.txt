Directory structure:
└── sangeetha2908-advance-time-series-forecasting-with-deep-reinforcement-learning/
    ├── README.md
    ├── baseline_strategy.py
    ├── data_generator.py
    ├── evaluation.py
    ├── report.md
    ├── trading_env.py
    └── train_agent.py

================================================
FILE: README.md
================================================
# advance-time-series-forecasting-with-deep-reinforcement-learning
Deep Reinforcement Learning is used to train a trading agent on synthetic financial time-series data. A custom Gym environment models Buy, Sell, Hold decisions. PPO learns to maximize returns vs a moving-average baseline using Sharpe ratio, drawdown, and cumulative profit metrics.



================================================
FILE: baseline_strategy.py
================================================
import pandas as pd

def sma_strategy(df, fast=10, slow=50):
    df = df.copy()
    df['ma_fast'] = df['price'].rolling(fast).mean()
    df['ma_slow'] = df['price'].rolling(slow).mean()
    df['signal'] = (df['ma_fast'] > df['ma_slow']).astype(int)
    df['market_return'] = df['price'].pct_change()
    df['strategy_return'] = df['market_return'] * df['signal'].shift(1)
    df['equity_curve'] = (1 + df['strategy_return'].fillna(0)).cumprod()
    return df



================================================
FILE: data_generator.py
================================================
import numpy as np
import pandas as pd

def generate_synthetic_data(n_steps=5000, seed=42):
    np.random.seed(seed)
    prices = [100]
    vol = 0.015
    long_term_mean = 100
    mean_revert_strength = 0.02
    for _ in range(n_steps):
        shock = np.random.randn() * vol
        mean_revert = mean_revert_strength * (long_term_mean - prices[-1])
        new_price = prices[-1] + mean_revert + shock
        prices.append(new_price)
    df = pd.DataFrame({'price': prices})
    df['return'] = df['price'].pct_change()
    df['ma_fast'] = df['price'].rolling(10).mean()
    df['ma_slow'] = df['price'].rolling(50).mean()
    df['rsi'] = 100 - (100 / (1 + df['return'].rolling(14).mean()))
    df = df.dropna()
    return df

if __name__ == '__main__':
    df = generate_synthetic_data()
    df.to_csv('synthetic_data.csv', index=False)
    print('Saved synthetic_data.csv', df.shape)



================================================
FILE: evaluation.py
================================================
import numpy as np
import pandas as pd
from stable_baselines3 import PPO
from trading_env import TradingEnv
from baseline_strategy import sma_strategy

def sharpe(returns):
    returns = np.array(returns)
    if returns.std() == 0:
        return 0.0
    return np.sqrt(252) * returns.mean() / returns.std()

def max_drawdown(equity):
    roll_max = equity.cummax()
    drawdown = (equity - roll_max) / roll_max
    return drawdown.min()

df = pd.read_csv('synthetic_data.csv')

# Baseline
base = sma_strategy(df)
base_equity = base['equity_curve'].dropna()
base_returns = base_equity.pct_change().dropna()
print('Baseline Sharpe:', sharpe(base_returns))
print('Baseline Total Return:', base_equity.iloc[-1] - 1)

# PPO (if trained)
try:
    model = PPO.load('models/ppo_final')
    env = TradingEnv(df)
    obs = env.reset()
    equity = [env.net_worth]
    done = False
    while not done:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, done, _ = env.step(int(action))
        equity.append(env.net_worth)
    returns = np.diff(equity) / equity[:-1]
    print('PPO Sharpe:', sharpe(returns))
    print('PPO Total Return:', equity[-1] / 10000 - 1)
except Exception as e:
    print('PPO evaluation skipped (model missing).', e)



================================================
FILE: report.md
================================================
# Advanced Time Series Forecasting with DRL

## Files included
- synthetic_data.csv (generated by data_generator.py)
- data_generator.py
- trading_env.py
- train_agent.py
- baseline_strategy.py
- evaluation.py
- images: price_series.png, moving_averages.png, rsi_plot.png

## How to run
1. Install requirements: pip install stable-baselines3 gym pandas numpy matplotlib
2. Generate data: python data_generator.py
3. Train agent: python train_agent.py
4. Evaluate: python evaluation.py

## Notes
- Training can take significant time; adjust timesteps for your machine.
- VecNormalize saves running statistics used for evaluation.



================================================
FILE: trading_env.py
================================================
import gym
import numpy as np

class TradingEnv(gym.Env):
    metadata = {'render.modes': ['human']}
    def __init__(self, df):
        super().__init__()
        self.df = df.reset_index(drop=True)
        self.current_step = 0
        self.initial_balance = 10000
        self.balance = self.initial_balance
        self.shares_held = 0
        self.net_worth = self.initial_balance
        self.max_steps = len(df) - 1
        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)
        self.action_space = gym.spaces.Discrete(3)
    def _get_state(self):
        row = self.df.iloc[self.current_step]
        return np.array([row.price, row.return, row.ma_fast, row.ma_slow, row.rsi, self.shares_held], dtype=np.float32)
    def step(self, action):
        price = self.df.iloc[self.current_step].price
        prev_worth = self.net_worth
        # BUY
        if action == 1 and self.balance >= price:
            self.shares_held += 1
            self.balance -= price
        # SELL
        elif action == 2 and self.shares_held > 0:
            self.shares_held -= 1
            self.balance += price
        self.net_worth = self.balance + self.shares_held * price
        reward = self.net_worth - prev_worth
        self.current_step += 1
        done = self.current_step >= self.max_steps
        return self._get_state(), reward, done, {}
    def reset(self):
        self.current_step = 0
        self.balance = self.initial_balance
        self.shares_held = 0
        self.net_worth = self.initial_balance
        return self._get_state()
    def render(self, mode='human'):
        print(f'Step: {self.current_step} Price: {self.df.iloc[self.current_step].price} NetWorth: {self.net_worth}')



================================================
FILE: train_agent.py
================================================
import os
import pandas as pd
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.monitor import Monitor
from trading_env import TradingEnv

SAVE_DIR = 'models'
os.makedirs(SAVE_DIR, exist_ok=True)
df = pd.read_csv('synthetic_data.csv')

def make_env():
    return Monitor(TradingEnv(df))

vec_env = DummyVecEnv([make_env])
vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)

eval_env = Monitor(TradingEnv(df))

model = PPO('MlpPolicy', vec_env, verbose=1, tensorboard_log='tb_logs', seed=42)
eval_callback = EvalCallback(eval_env, best_model_save_path=SAVE_DIR, log_path=SAVE_DIR, eval_freq=10000, n_eval_episodes=5)
model.learn(total_timesteps=200000, callback=eval_callback)
model.save(os.path.join(SAVE_DIR, 'ppo_final'))
vec_env.save(os.path.join(SAVE_DIR, 'vec_normalize.pkl'))
print('Training finished and saved.')


